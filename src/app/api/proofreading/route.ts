import { vision } from '@google-cloud/vision';

// Initialize the client using the environment variable
const client = new vision.ImageAnnotatorClient();

export async function POST(request: Request) {
  try {
    const { imageUrl } = await request.json();
    
    if (!imageUrl) {
      throw new Error('No image URL provided');
    }

    const base64Data = imageUrl.split(',')[1];

    // ðŸ§  Step 1: Google Vision OCR
    const [visionResponse] = await client.textDetection({
      image: { content: Buffer.from(base64Data, 'base64') },
    });

    const textAnnotations = visionResponse.textAnnotations;
    const extractedText = textAnnotations?.[0]?.description?.trim() || '';

    if (!extractedText) {
      throw new Error('No text extracted from the image');
    }

    console.log('Extracted text:', extractedText);

    // ðŸ§  Step 2: Groq grammar correction
    const grammarFix = await groq.chat.completions.create({
      model: "meta-llama/llama-4-maverick-32k",
      messages: [
        {
          role: "user",
          content: Please proofread and correct the following text for grammar, spelling, and clarity:\n\n${extractedText}
        }
      ],
      temperature: 0.2,
      max_tokens: 2048,
    });

    return Response.json({
      extractedText,
      accuracyAnalysis: grammarFix.choices[0].message.content.trim(),
    });

  } catch (error) {
    console.error('Proofreading API error:', error.message || error);
    return Response.json({ error: Failed to extract text: ${error.message} }, { status:Â 500Â });
Â Â }
}
